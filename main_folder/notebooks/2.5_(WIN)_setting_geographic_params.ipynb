{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c42ec3",
   "metadata": {},
   "source": [
    "### 1. Первичная выгрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90982f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reestr_new_TOTAL.csv').copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ce094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Проверяем доступность GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Используем: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76924bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "''' Удаление столбца \"ЖК\", так как это наименование ЖК '''\n",
    "if 'ЖК' in df.columns:\n",
    "    df = df.drop('ЖК', axis=1)\n",
    "    \n",
    "    \n",
    "''' Проверка на пропущенные значения '''\n",
    "print('Пропущенные значения по столбцам:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "''' # Проверка на бесконечно малые значения и их обнуление '''\n",
    "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    small_values = np.abs(df[col]) < 1e-10\n",
    "    if small_values.any():\n",
    "        print(f\"- Обнаружены бесконечно малые значения в столбце '{col}': {small_values.sum()} значений\")\n",
    "        df.loc[small_values, col] = 0\n",
    "        print(f\"  Бесконечно малые значения в столбце '{col}' обнулены\")\n",
    "print()        \n",
    "\n",
    "\n",
    "''' Проверка на слишком большие значения (больше 5 млн) '''\n",
    "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    large_values = df[col] > 5_000_000\n",
    "    if large_values.any():\n",
    "        print(f\"- Обнаружены слишком большие значения (>5 млн) в столбце '{col}':\")\n",
    "        print(df.loc[large_values, col])\n",
    "        \n",
    "    # Если это целевая переменная price_per_sqm, просто выводим информацию\n",
    "    if col == 'price_per_sqm':\n",
    "        print(f\"- Количество выбросов в целевой переменной 'price_per_sqm': {large_values.sum()}\")\n",
    "        # Удаляем строки с очень большими ценами\n",
    "        df = df[~large_values]\n",
    "        print('  Строки с очень большими ценами удалены')\n",
    "print()\n",
    "\n",
    "            \n",
    "''' Проверка на NaN, бесконечности и очень большие значения в price_per_sqm '''\n",
    "if 'price_per_sqm' in df.columns:\n",
    "    \n",
    "    # Проверка на NaN\n",
    "    nan_prices = df['price_per_sqm'].isna()\n",
    "    if nan_prices.any():\n",
    "        print(f\"- Обнаружены NaN значения в 'price_per_sqm': {nan_prices.sum()} значений\")\n",
    "        # Удаляем строки с NaN ценами\n",
    "        df = df[~nan_prices]\n",
    "        print('  Строки с NaN ценами удалены\\n')\n",
    " \n",
    "        \n",
    "    # Проверка на бесконечности\n",
    "    inf_prices = ~np.isfinite(df['price_per_sqm'])\n",
    "    if inf_prices.any():\n",
    "        print(f\"- Обнаружены бесконечные значения в 'price_per_sqm': {inf_prices.sum()} значений\")\n",
    "        # Удаляем строки с бесконечными ценами\n",
    "        df = df[~inf_prices]\n",
    "        print('  Строки с бесконечными ценами удалены\\n') \n",
    "        \n",
    "    # Проверка на отрицательные значения\n",
    "    negative_prices = df['price_per_sqm'] <= 0\n",
    "    if negative_prices.any():\n",
    "        print(f\"- Обнаружены отрицательные или нулевые значения в 'price_per_sqm': {negative_prices.sum()} значений\")\n",
    "        # Удаляем строки с отрицательными ценами\n",
    "        df = df[~negative_prices]\n",
    "        print('  Строки с отрицательными или нулевыми ценами удалены\\n')\n",
    "    \n",
    "    print()\n",
    "\n",
    "\n",
    "''' Автоматическое определение числовых столбцов и заполнение пропусков медианой '''\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "for col in numeric_columns:\n",
    "    missing_values = df[col].isna()\n",
    "    if missing_values.any():\n",
    "        print(f\"Заполнение пропусков в столбце '{col}': {missing_values.sum()} значений\")\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "\n",
    "''' Автоматическое определение категориальных столбцов и заполнение пропусков модой '''\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in categorical_columns:\n",
    "    missing_values = df[col].isna()\n",
    "    if missing_values.any():\n",
    "        # Находим наиболее часто встречающееся значение (моду)\n",
    "        most_frequent = df[col].mode()[0]\n",
    "        print(f\"Заполнение пропусков в столбце '{col}': {missing_values.sum()} значений значением '{most_frequent}'\")\n",
    "        df[col] = df[col].fillna(most_frequent)\n",
    "\n",
    "\n",
    "''' Заполнение пропусков в столбцах застройщиков '''\n",
    "developer_columns = [col for col in df.columns if col.startswith('developer_')]\n",
    "for col in developer_columns:\n",
    "    if col in df.columns:\n",
    "        missing_values = df[col].isna()\n",
    "        if missing_values.any():\n",
    "            print(f\"Заполнение пропусков в столбце '{col}': {missing_values.sum()} значений\")\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "\n",
    "''' Заполнение пропусков в столбцах банков '''\n",
    "bank_columns = [col for col in df.columns if col.startswith('bank_')]\n",
    "for col in bank_columns:\n",
    "    if col in df.columns:\n",
    "        missing_values = df[col].isna()\n",
    "        if missing_values.any():\n",
    "            print(f\"Заполнение пропусков в столбце '{col}': {missing_values.sum()} значений\")\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "            \n",
    "''' Проверка на оставшиеся пропуски '''\n",
    "remaining_nulls = df.isnull().sum()\n",
    "if remaining_nulls.sum() > 0:\n",
    "    print(\"\\nОставшиеся пропущенные значения после обработки:\")\n",
    "    print(remaining_nulls[remaining_nulls > 0])\n",
    "    \n",
    "    # Заполняем оставшиеся пропуски медианой для числовых и наиболее частым значением для категориальных\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            \n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "      \n",
    "                \n",
    "''' Базовая статистика по целевой переменной '''\n",
    "if 'price_per_sqm' in df.columns:\n",
    "    print(\"\\nСтатистика по целевой переменной 'price_per_sqm':\")\n",
    "    print(df['price_per_sqm'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6354821f",
   "metadata": {},
   "source": [
    "### 2. Более глубл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f566c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем доступность CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используется устройство: {device}\")\n",
    "\n",
    "\n",
    "# Функция для расчета расстояния по формуле гаверсинусов с использованием PyTorch\n",
    "'''\n",
    "Расчет расстояний между точками с использованием PyTorch.\n",
    "[] Args:\n",
    "    - lat1, lon1: координаты первой точки (тензоры)\n",
    "    - lat2, lon2: координаты второй точки (тензоры)  \n",
    "[] Returns:\n",
    "    - тензор с расстояниями в километрах\n",
    "'''\n",
    "def haversine_distance_batch(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    # Радиус Земли в километрах\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Преобразуем градусы в радианы\n",
    "    lat1_rad = torch.deg2rad(lat1)\n",
    "    lon1_rad = torch.deg2rad(lon1)\n",
    "    lat2_rad = torch.deg2rad(lat2)\n",
    "    lon2_rad = torch.deg2rad(lon2)\n",
    "    \n",
    "    # Разница в координатах\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Формула гаверсинусов\n",
    "    a = torch.sin(dlat/2)**2 + torch.cos(lat1_rad) * torch.cos(lat2_rad) * torch.sin(dlon/2)**2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "\n",
    "# Функция находит ближайшие аналоги для каждого объекта с максимальным использованием GPU и применяет пространственное взвешивание цен аналогов\n",
    "'''\n",
    "[] Args:\n",
    "    - df: исходный датафрейм\n",
    "    - num_analogs: количество аналогов для поиска\n",
    "    - batch_size: размер батча для обработки на GPU\n",
    "    - sigma: параметр для экспоненциального взвешивания расстояний  \n",
    "[] Returns:\n",
    "    - DataFrame: исходный датафрейм с добавленными столбцами для аналогов и пространственными метриками\n",
    "'''\n",
    "def find_nearest_analogs_gpu_optimized(df, num_analogs=10, batch_size=1000, sigma=2.0):\n",
    "\n",
    "    print(f\"Поиск ближайших аналогов для {len(df)} объектов с GPU-оптимизацией...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Создаем копию исходного датафрейма\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Преобразуем строковые данные в числовые идентификаторы\n",
    "    encoder = LabelEncoder()\n",
    "    residential_complex_encoded = encoder.fit_transform(df['residential_complex'])\n",
    "    \n",
    "    # Создаем столбцы для аналогов\n",
    "    for i in range(1, num_analogs + 1):\n",
    "        result_df[f'analog_{i}_price_per_sqm'] = np.nan\n",
    "        result_df[f'analog_{i}_distance'] = np.nan\n",
    "        result_df[f'analog_{i}_lat'] = np.nan\n",
    "        result_df[f'analog_{i}_lng'] = np.nan\n",
    "        result_df[f'analog_{i}_weight'] = np.nan\n",
    "    \n",
    "    # Переносим все данные на GPU для ускорения\n",
    "    indices_tensor = torch.tensor(df.index.values, device=device)\n",
    "    class_tensor = torch.tensor(df['class'].values, device=device)\n",
    "    finishing_tensor = torch.tensor(df['finishing'].values, device=device)\n",
    "    region_tensor = torch.tensor(df['region'].values, device=device)\n",
    "    zone_tensor = torch.tensor(df['zone'].values, device=device)\n",
    "    renovation_tensor = torch.tensor(df['renovation'].values, device=device)\n",
    "    premises_type_tensor = torch.tensor(df['premises_type'].values, device=device)\n",
    "    reg_year_tensor = torch.tensor(df['registration_date_year'].values, device=device)\n",
    "    rc_tensor = torch.tensor(residential_complex_encoded, device=device)\n",
    "    lat_tensor = torch.tensor(df['lat'].values, device=device)\n",
    "    lng_tensor = torch.tensor(df['lng'].values, device=device)\n",
    "    price_tensor = torch.tensor(df['price_per_sqm'].values, device=device)\n",
    "    \n",
    "    # Обрабатываем датафрейм по частям для экономии памяти\n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Обработка батчей\"):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_indices = df.index[start_idx:end_idx]\n",
    "        \n",
    "        # Для каждого объекта в батче\n",
    "        for idx in batch_indices:\n",
    "            i = df.index.get_loc(idx)\n",
    "            \n",
    "            # Получаем параметры текущего объекта\n",
    "            current_class = class_tensor[i]\n",
    "            current_finishing = finishing_tensor[i]\n",
    "            current_region = region_tensor[i]\n",
    "            current_zone = zone_tensor[i]\n",
    "            current_renovation = renovation_tensor[i]\n",
    "            current_premises_type = premises_type_tensor[i]\n",
    "            current_reg_year = reg_year_tensor[i]\n",
    "            current_rc = rc_tensor[i]\n",
    "            current_lat = lat_tensor[i]\n",
    "            current_lng = lng_tensor[i]\n",
    "            current_idx = indices_tensor[i]\n",
    "            \n",
    "            # Фильтруем потенциальные аналоги по критериям - все операции на GPU\n",
    "            mask = (\n",
    "                    (class_tensor == current_class) &\n",
    "                    (finishing_tensor == current_finishing) &\n",
    "                    (region_tensor == current_region) &\n",
    "                    (zone_tensor == current_zone) &\n",
    "                    (renovation_tensor == current_renovation) &\n",
    "                    (premises_type_tensor == current_premises_type) &\n",
    "                    (reg_year_tensor <= current_reg_year) &\n",
    "                    (reg_year_tensor >= current_reg_year - 1) &\n",
    "                    (rc_tensor != current_rc) & \n",
    "                    (indices_tensor != current_idx)\n",
    "                    )\n",
    "            \n",
    "            # Если аналогов недостаточно, ослабляем критерии\n",
    "            if torch.sum(mask) < num_analogs:\n",
    "                mask = (\n",
    "                        (class_tensor == current_class) &\n",
    "                        (region_tensor == current_region) &\n",
    "                        (premises_type_tensor == current_premises_type) &\n",
    "                        (reg_year_tensor <= current_reg_year) &\n",
    "                        (reg_year_tensor >= current_reg_year - 2) &\n",
    "                        (rc_tensor != current_rc) & \n",
    "                        (indices_tensor != current_idx)\n",
    "                        )\n",
    "            \n",
    "            # Если все еще недостаточно, еще больше ослабляем критерии\n",
    "            if torch.sum(mask) < num_analogs:\n",
    "                mask = (\n",
    "                        (class_tensor == current_class) &\n",
    "                        (premises_type_tensor == current_premises_type) &\n",
    "                        (rc_tensor != current_rc) & \n",
    "                        (indices_tensor != current_idx)\n",
    "                        )\n",
    "            \n",
    "            # Если нашли потенциальные аналоги\n",
    "            mask_sum = torch.sum(mask)\n",
    "            if mask_sum > 0:\n",
    "                \n",
    "                # Получаем координаты, цены и индексы аналогов - все на GPU\n",
    "                analog_lats = lat_tensor[mask]\n",
    "                analog_lngs = lng_tensor[mask]\n",
    "                analog_prices = price_tensor[mask]\n",
    "                analog_indices = indices_tensor[mask]\n",
    "                \n",
    "                # Рассчитываем расстояния до всех потенциальных аналогов\n",
    "                current_lat_expanded = current_lat.expand_as(analog_lats)\n",
    "                current_lng_expanded = current_lng.expand_as(analog_lngs)\n",
    "                distances = haversine_distance_batch(current_lat_expanded, current_lng_expanded, analog_lats, analog_lngs)\n",
    "                \n",
    "                # Если аналогов больше, чем нужно, выбираем ближайшие\n",
    "                if mask_sum > num_analogs:\n",
    "                    # Используем topk для выбора ближайших аналогов - операция на GPU\n",
    "                    _, indices = torch.topk(distances, num_analogs, largest=False)\n",
    "                    selected_distances = torch.gather(distances, 0, indices)\n",
    "                    selected_prices = torch.gather(analog_prices, 0, indices)\n",
    "                    selected_lats = torch.gather(analog_lats, 0, indices)\n",
    "                    selected_lngs = torch.gather(analog_lngs, 0, indices)\n",
    "                    selected_indices = torch.gather(analog_indices, 0, indices)\n",
    "                else:\n",
    "                    selected_distances = distances\n",
    "                    selected_prices = analog_prices\n",
    "                    selected_lats = analog_lats\n",
    "                    selected_lngs = analog_lngs\n",
    "                    selected_indices = analog_indices\n",
    "                \n",
    "                # Рассчитываем экспоненциальные веса на основе расстояния - на GPU\n",
    "                weights = torch.exp(-selected_distances / sigma)\n",
    "                weights = weights / torch.sum(weights) \n",
    "                \n",
    "                # Переносим результаты на CPU\n",
    "                distances_np = selected_distances.cpu().numpy()\n",
    "                prices_np = selected_prices.cpu().numpy()\n",
    "                lats_np = selected_lats.cpu().numpy()\n",
    "                lngs_np = selected_lngs.cpu().numpy()\n",
    "                weights_np = weights.cpu().numpy()\n",
    "                \n",
    "                # Заполняем столбцы для аналогов\n",
    "                for j in range(min(num_analogs, len(distances_np))):\n",
    "                    result_df.at[idx, f'analog_{j+1}_price_per_sqm'] = prices_np[j]\n",
    "                    result_df.at[idx, f'analog_{j+1}_distance'] = distances_np[j]\n",
    "                    result_df.at[idx, f'analog_{j+1}_lat'] = lats_np[j]\n",
    "                    result_df.at[idx, f'analog_{j+1}_lng'] = lngs_np[j]\n",
    "                    result_df.at[idx, f'analog_{j+1}_weight'] = weights_np[j]\n",
    "    \n",
    "    # Вычисляем агрегированные метрики\n",
    "    print(\"Расчет агрегированных метрик...\")\n",
    "    \n",
    "    # Подготавливаем названия столбцов\n",
    "    analog_price_cols = [f'analog_{i}_price_per_sqm' for i in range(1, num_analogs + 1)]\n",
    "    analog_distance_cols = [f'analog_{i}_distance' for i in range(1, num_analogs + 1)]\n",
    "    analog_weight_cols = [f'analog_{i}_weight' for i in range(1, num_analogs + 1)]\n",
    "    analog_lat_cols = [f'analog_{i}_lat' for i in range(1, num_analogs + 1)]\n",
    "    analog_lng_cols = [f'analog_{i}_lng' for i in range(1, num_analogs + 1)]\n",
    "    \n",
    "    # Обрабатываем строки с отсутствующими аналогами\n",
    "    for col in analog_price_cols + analog_distance_cols + analog_weight_cols + analog_lat_cols + analog_lng_cols:\n",
    "        result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "    \n",
    "    # 1. Базовые агрегированные метрики\n",
    "    result_df['analogs_mean_price'] = result_df[analog_price_cols].mean(axis=1)\n",
    "    result_df['analogs_median_price'] = result_df[analog_price_cols].median(axis=1)\n",
    "    result_df['analogs_min_price'] = result_df[analog_price_cols].min(axis=1)\n",
    "    result_df['analogs_max_price'] = result_df[analog_price_cols].max(axis=1)\n",
    "    result_df['analogs_std_price'] = result_df[analog_price_cols].std(axis=1)\n",
    "    result_df['analogs_mean_distance'] = result_df[analog_distance_cols].mean(axis=1)\n",
    "    \n",
    "    # 2. Взвешенные метрики на основе расстояния\n",
    "    # Используем векторизованные операции pandas\n",
    "    result_df['analogs_weighted_mean_price'] = (\n",
    "        result_df[analog_price_cols].values * result_df[analog_weight_cols].values\n",
    "    ).sum(axis=1)\n",
    "    \n",
    "    # 3. Пространственные характеристики распределения аналогов\n",
    "    result_df['analogs_lat_std'] = result_df[analog_lat_cols].std(axis=1)\n",
    "    result_df['analogs_lng_std'] = result_df[analog_lng_cols].std(axis=1)\n",
    "    \n",
    "    # Направление распределения аналогов (север-юг или восток-запад)\n",
    "    result_df['analogs_direction_ratio'] = result_df['analogs_lat_std'] / (result_df['analogs_lng_std'] + 0.001)\n",
    "    \n",
    "    # 4. Рассчитываем отклонение цены объекта от взвешенной средней аналогов\n",
    "    result_df['price_deviation_from_weighted_analogs'] = (result_df['price_per_sqm'] - result_df['analogs_weighted_mean_price']) / result_df['analogs_weighted_mean_price']\n",
    "    \n",
    "    # 5. Градиент цен - ИСПРАВЛЕННАЯ ВЕРСИЯ с использованием sklearn\n",
    "    print(\"Расчет градиентов цен с использованием sklearn...\")\n",
    "    \n",
    "    # Подготавливаем столбцы для градиентов\n",
    "    result_df['price_gradient_lat'] = 0.0  # Инициализируем нулями вместо NaN\n",
    "    result_df['price_gradient_lng'] = 0.0\n",
    "    result_df['price_gradient_magnitude'] = 0.0\n",
    "    result_df['price_gradient_direction'] = 0.0\n",
    "    \n",
    "    # Обрабатываем датафрейм по частям для экономии памяти\n",
    "    for start_idx in tqdm(range(0, len(result_df), batch_size), desc=\"Расчет градиентов по батчам\"):\n",
    "        end_idx = min(start_idx + batch_size, len(result_df))\n",
    "        batch_indices = result_df.index[start_idx:end_idx]\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            # Получаем данные текущего объекта и его аналогов\n",
    "            row = result_df.loc[idx]\n",
    "            \n",
    "            # Собираем данные для расчета градиента\n",
    "            X_data = []\n",
    "            y_data = []\n",
    "            \n",
    "            for i in range(1, num_analogs + 1):\n",
    "                \n",
    "                # Проверяем, что все необходимые данные аналога присутствуют\n",
    "                if (not pd.isna(row[f'analog_{i}_lat']) and \n",
    "                    not pd.isna(row[f'analog_{i}_lng']) and \n",
    "                    not pd.isna(row[f'analog_{i}_price_per_sqm'])):\n",
    "                    \n",
    "                    # Разница в координатах (аналог - объект)\n",
    "                    lat_diff = row[f'analog_{i}_lat'] - row['lat']\n",
    "                    lng_diff = row[f'analog_{i}_lng'] - row['lng']\n",
    "                    \n",
    "                    # Разница в цене\n",
    "                    price_diff = row[f'analog_{i}_price_per_sqm'] - row['price_per_sqm']\n",
    "                    \n",
    "                    X_data.append([lat_diff, lng_diff])\n",
    "                    y_data.append(price_diff)\n",
    "            \n",
    "            # Если есть достаточно данных для расчета градиента\n",
    "            if len(X_data) >= 3: \n",
    "                try:\n",
    "                    # Используем sklearn LinearRegression для надежности\n",
    "                    reg = LinearRegression(fit_intercept=False) \n",
    "                    reg.fit(X_data, y_data)\n",
    "                    \n",
    "                    # Получаем коэффициенты градиента\n",
    "                    gradient_lat = reg.coef_[0]\n",
    "                    gradient_lng = reg.coef_[1]\n",
    "                    \n",
    "                    # Рассчитываем магнитуду и направление градиента\n",
    "                    magnitude = np.sqrt(gradient_lat**2 + gradient_lng**2)\n",
    "                    direction = np.arctan2(gradient_lat, gradient_lng) * 180 / np.pi\n",
    "                    \n",
    "                    # Сохраняем результаты\n",
    "                    result_df.at[idx, 'price_gradient_lat'] = gradient_lat\n",
    "                    result_df.at[idx, 'price_gradient_lng'] = gradient_lng\n",
    "                    result_df.at[idx, 'price_gradient_magnitude'] = magnitude\n",
    "                    result_df.at[idx, 'price_gradient_direction'] = direction\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # В случае ошибки просто пропускаем (оставляем нули)\n",
    "                    print(f\"Ошибка при расчете градиента для {idx}: {e}\")\n",
    "    \n",
    "    # 6. Добавляем различные комбинации признаков с векторизацией\n",
    "    print(\"Расчет комбинированных признаков...\")\n",
    "    \n",
    "    # Взаимодействие между градиентом цены и расстоянием\n",
    "    result_df['gradient_distance_interaction'] = result_df['price_gradient_magnitude'] * result_df['analogs_mean_distance']\n",
    "    \n",
    "    # Относительное положение объекта в градиенте цен - векторизованная реализация\n",
    "    result_df['relative_position_in_gradient'] = 0\n",
    "    \n",
    "    # Используем векторизацию для ускорения\n",
    "    for i in range(1, num_analogs + 1):\n",
    "        # Проекция вектора от аналога к объекту на вектор градиента\n",
    "        lat_diff = result_df['lat'] - result_df[f'analog_{i}_lat']\n",
    "        lng_diff = result_df['lng'] - result_df[f'analog_{i}_lng']\n",
    "        \n",
    "        # Нормализованный вектор градиента\n",
    "        grad_magnitude = result_df['price_gradient_magnitude']\n",
    "        grad_lat = result_df['price_gradient_lat'] / (grad_magnitude + 0.001)\n",
    "        grad_lng = result_df['price_gradient_lng'] / (grad_magnitude + 0.001)\n",
    "        \n",
    "        # Скалярное произведение\n",
    "        projection = lat_diff * grad_lat + lng_diff * grad_lng\n",
    "        result_df['relative_position_in_gradient'] += projection * result_df[f'analog_{i}_weight']\n",
    "    \n",
    "    # Заполняем NaN в агрегированных метриках\n",
    "    agg_cols = [\n",
    "                'analogs_mean_price', 'analogs_median_price', 'analogs_min_price', \n",
    "                'analogs_max_price', 'analogs_std_price', 'analogs_mean_distance',\n",
    "                'analogs_weighted_mean_price', 'analogs_lat_std', 'analogs_lng_std',\n",
    "                'analogs_direction_ratio', 'price_gradient_lat', 'price_gradient_lng',\n",
    "                'price_gradient_magnitude', 'price_gradient_direction',\n",
    "                'price_deviation_from_weighted_analogs', 'gradient_distance_interaction',\n",
    "                'relative_position_in_gradient'\n",
    "                ]\n",
    "    \n",
    "    # Заменяем NaN медианными значениями (более робастно, чем среднее)                       \n",
    "    for col in agg_cols:\n",
    "        median_value = result_df[col].median()\n",
    "        if pd.isna(median_value):\n",
    "            median_value = 0\n",
    "        result_df[col] = result_df[col].fillna(median_value)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Обработка завершена за {end_time - start_time:.2f} секунд.\")\n",
    "    return result_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
